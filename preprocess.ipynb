{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess and save input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b402374d5f4e24a6451b901d5cc86c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting train filenames...\n",
      "Done collecting filenames!\n",
      "Starting extracting features from train video...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bed5ce81b594ad1b07c78d2e9dc8a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19520.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting val filenames...\n",
      "Done collecting filenames!\n",
      "Starting extracting features from val video...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d0d5584f5943bab41edff7a5fbcba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting test filenames...\n",
      "Done collecting filenames!\n",
      "Starting extracting features from test video...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc25d50733c14eb084014422a5193d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from preprocess import * \n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy.io import loadmat, savemat\n",
    "import imutils\n",
    "from imutils import face_utils\n",
    "import dlib\n",
    "import cv2\n",
    "# import menpo\n",
    "# import menpofit\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "data_dir = './data/lipread_20_mp4/'\n",
    "modes = ['train', 'val', 'test']\n",
    " \n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('./shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "for mode in tqdm(modes): \n",
    "    \n",
    "    # Data is organized: data_dir > WORD > MODE > WORD_ID.txt\n",
    "    print(f'Collecting {mode} filenames...')\n",
    "    data_files = [] \n",
    "\n",
    "    for word in os.listdir(data_dir):  \n",
    "\n",
    "        for path in glob.glob(os.path.join(data_dir,word,mode,'*.txt')): \n",
    "            fname = os.path.splitext(os.path.basename(path))[0]\n",
    "            data_files.append(fname) \n",
    "        \n",
    "    print('Done collecting filenames!')   \n",
    "    \n",
    "    print(f'Starting extracting features from {mode} video...')  \n",
    "    \n",
    "    for indx, fname in enumerate(tqdm(data_files)): \n",
    "\n",
    "        label_vocab = fname.split('_')[0]\n",
    "\n",
    "#         print(fname)\n",
    "\n",
    "        vid_path = f'{data_dir}/{label_vocab}/{mode}/{fname}.mp4'\n",
    "#         arr_save_path = f'{data_dir}/{label_vocab}/{mode}/{fname}.npy'\n",
    "        arr_save_path = f'{data_dir}/{label_vocab}/{mode}/{fname}_mouth_frames.npy'\n",
    "    \n",
    "        if os.path.isfile(arr_save_path): \n",
    "            continue\n",
    "\n",
    "        # features = dlib_features_from_vid_path(vid_path, detector, predictor, normalize=True)       \n",
    "        mouth_frames = dlib_get_frames_mouth_only(vid_path, detector, predictor, normalize=True)\n",
    "        \n",
    "#         np.save(arr_save_path, features)\n",
    "        np.save(arr_save_path, mouth_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 68, 2)\n"
     ]
    }
   ],
   "source": [
    "# Debugging cell \n",
    "data_dir = './data/lipread_20_mp4/'\n",
    "mode = 'train'\n",
    "label_vocab = 'ABOUT'\n",
    "fname ='ABOUT_00001'\n",
    "\n",
    "load_path = f'{data_dir}/{label_vocab}/{mode}/{fname}.npy'\n",
    "features = np.load(load_path)\n",
    "\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to extract facial features using supervised descent fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import menpo\n",
    "import menpofit\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "\n",
    "from menpodetect.dlib.detect import DlibDetector\n",
    "import dlib\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "model_path = './keypoint_model.pkl.gz'\n",
    "\n",
    "#load Bob model\n",
    "with gzip.open(model_path) as f:\n",
    "    bob_model = pickle.load(f)\n",
    "\n",
    "#create face detector\n",
    "ff_detector = DlibDetector(dlib.get_frontal_face_detector())\n",
    "\n",
    "def features_from_vid_path(path, ff_detector, model):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    feature_vecs = []\n",
    "\n",
    "    frame_ctr = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        feature_vecs.append(features_from_frame(frame, ff_detector, model))\n",
    "    return feature_vecs\n",
    "    \n",
    "def features_from_frame(frame, ff_detector, model):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_img = menpo.image.Image.init_from_channels_at_back(frame)\n",
    "    bboxes = ff_detector(frame_img)\n",
    "    fr = model.fit_from_bb(frame_img, bboxes[0], max_iters=100)\n",
    "    flat_vec = fr.final_shape.as_vector()\n",
    "    #return normalize_vec(flat_vec, fr)\n",
    "    return format_vec(flat_vec)\n",
    "\n",
    "def format_vec(vec):\n",
    "    x_pts, y_pts = vec[::2], vec[1::2]\n",
    "    return np.array([[x, y] for x, y in zip(x_pts, y_pts)])\n",
    "    \n",
    "# Takes a flat vector and normalizes the x and y axes to fit in 0,1\n",
    "def normalize_vec(vec, fr):\n",
    "    x_pts, y_pts = vec[::2], vec[1::2]\n",
    "    min_bounds, max_bounds = fr.final_shape.bounds()\n",
    "\n",
    "    x_range = max_bounds[0] - min_bounds[0]\n",
    "    y_range = max_bounds[1] - min_bounds[1]\n",
    "\n",
    "    x_norm = (x_pts - min_bounds[0]) / x_range\n",
    "    y_norm = (y_pts - min_bounds[1]) / y_range\n",
    "    return np.array([[x, y] for x, y in zip(x_norm, y_norm)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Debugging cell \n",
    "lrs2_path = '/Volumes/T7/lrs2/'\n",
    "features_from_vid_path(f'{lrs2_path}/mvlrs_v1/main/5535415699068794046/00001.mp4', ff_detector, bob_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
